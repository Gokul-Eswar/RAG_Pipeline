version: "3.8"

services:
  # Kafka-compatible streaming (Redpanda)
  redpanda:
    image: vectorized/redpanda:latest
    command: ["redpanda", "start", "--overprovisioned", "--smp", "1", "--memory", "1G", "--reserve-memory", "0M", "--check", "false"]
    ports:
      - "9092:9092"
      - "9644:9644"
    volumes:
      - redpanda_data:/var/lib/redpanda/data

  # Vector database (Qdrant)
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # Graph database (Neo4j)
  neo4j:
    image: neo4j:5
    environment:
      NEO4J_AUTH: "neo4j/test"
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data

  # Relational DB for orchestration / metadata
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  # Airflow (light scaffold) â€” replace with your preferred orchestration image/config
  airflow:
    image: apache/airflow:2.9.1
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    ports:
      - "8080:8080"
    command: bash -c "airflow db upgrade || true; airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true; airflow webserver"

  # Minimal FastAPI app for the API surface
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
      - neo4j
      - redpanda
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BROKER=redpanda:9092
      - NEO4J_URI=bolt://neo4j:7687
      - QDRANT_HOST=qdrant
      - LOG_LEVEL=INFO

  # Background Processing Pipeline
  pipeline:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    command: ["python", "-m", "src.processing.pipeline"]
    depends_on:
      - redpanda
      - qdrant
      - neo4j
    environment:
      - KAFKA_BROKER=redpanda:9092
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_AUTH=neo4j/test
      - QDRANT_HOST=qdrant
      - LOG_LEVEL=INFO
    restart: on-failure

  # Spark Master for distributed processing
  spark-master:
    image: bitnami/spark:3.5.0
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8081:8081"
    volumes:
      - spark_data:/opt/spark-apps

  # Spark Worker 1 for distributed processing
  spark-worker-1:
    image: bitnami/spark:3.5.0
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8082:8081"
    volumes:
      - spark_data:/opt/spark-apps

  # Spark Worker 2 for distributed processing (optional, can remove if not needed)
  spark-worker-2:
    image: bitnami/spark:3.5.0
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8083:8081"
    volumes:
      - spark_data:/opt/spark-apps

volumes:
  redpanda_data:
  qdrant_data:
  neo4j_data:
  pgdata:
  spark_data:

# Notes:
# - This compose file provides a developer-friendly stack with Spark cluster for distributed processing
# - Spark Master: spark://spark-master:7077 (Web UI: http://localhost:8081)
# - Spark Worker 1: http://localhost:8082
# - Spark Worker 2: http://localhost:8083
# - Some images (Airflow, Spark) may require additional configuration for production usage
# - Replace the `api` build with your production image or extend the Dockerfile
# - To disable spark-worker-2 for lighter environments, comment out the service definition